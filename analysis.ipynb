{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8473b3-2f43-4e2e-89ed-935bb8c2b65c",
   "metadata": {},
   "source": [
    "# Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ecd115-96d7-42f4-b349-4510980cafa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1c5c0-726c-42f8-846b-ab4253456f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afca5e6-b42d-4928-adc1-5523210237f0",
   "metadata": {},
   "source": [
    "# Add new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de347c5-a534-4823-ae27-eb4dd680a953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e88ab.jpg</td>\n",
       "      <td>w_f48451c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f9222.jpg</td>\n",
       "      <td>w_c3d896a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00029d126.jpg</td>\n",
       "      <td>w_20df2c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00050a15a.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005c1ef8.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image         Id\n",
       "0  0000e88ab.jpg  w_f48451c\n",
       "1  0001f9222.jpg  w_c3d896a\n",
       "2  00029d126.jpg  w_20df2c5\n",
       "3  00050a15a.jpg  new_whale\n",
       "4  0005c1ef8.jpg  new_whale"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_map_df = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "train_map_df['Id'] = train_map_df['Id'].astype('category')\n",
    "train_map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb625b90-2e47-48fb-afee-f1ceb378d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(len(train_map_df['Id'].unique()))(x) # A Dense classifier with a each class from our training set\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14188bf3-7d29-448c-90d5-71e8107662fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5005)              2567565   \n",
      "=================================================================\n",
      "Total params: 17,282,253\n",
      "Trainable params: 2,567,565\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6120f82-73a6-436e-897e-2c06ea216b38",
   "metadata": {},
   "source": [
    "# Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8902e845-5436-4151-92cf-af5fb69c5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a categorical problem so need to use CategoricalCrossentropy and CategoricalAccuracy\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcec5d-4682-4f04-a256-5d89088db3b8",
   "metadata": {},
   "source": [
    "# Augment the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fa14f1-fd00-42c1-9b67-0d718184ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        validation_split=0.2)  # 80/20 split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f288d0-b3a5-48ac-980b-9f939e84ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20289 validated image filenames belonging to 5005 classes.\n",
      "Found 5072 validated image filenames belonging to 5005 classes.\n"
     ]
    }
   ],
   "source": [
    "train_it = datagen.flow_from_dataframe(train_map_df,\n",
    "                                       directory=os.path.join('data', 'train'), \n",
    "                                       x_col='Image',\n",
    "                                       y_col='Id',\n",
    "                                       target_size=(224, 224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode='categorical', \n",
    "                                       batch_size=8,\n",
    "                                       subset='training')\n",
    "valid_it = datagen.flow_from_dataframe(train_map_df,\n",
    "                                       directory=os.path.join('data', 'train'), \n",
    "                                       x_col='Image',\n",
    "                                       y_col='Id',\n",
    "                                       target_size=(224, 224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode='categorical', \n",
    "                                       batch_size=8,\n",
    "                                       subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e384ab-26f0-4e4c-badd-8601ee491696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 [==============================] - 16s 1s/step - loss: 7.7687 - categorical_accuracy: 0.3333 - val_loss: 9.3489 - val_categorical_accuracy: 0.2812\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 7.9011 - categorical_accuracy: 0.3438 - val_loss: 8.1808 - val_categorical_accuracy: 0.4062\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 8.1152 - categorical_accuracy: 0.3958 - val_loss: 8.7174 - val_categorical_accuracy: 0.3125\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 9.0516 - categorical_accuracy: 0.3646 - val_loss: 8.0855 - val_categorical_accuracy: 0.4375\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 8.7274 - categorical_accuracy: 0.3438 - val_loss: 7.1633 - val_categorical_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 8.3182 - categorical_accuracy: 0.3854 - val_loss: 9.3150 - val_categorical_accuracy: 0.3438\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 8.8616 - categorical_accuracy: 0.4062 - val_loss: 9.5607 - val_categorical_accuracy: 0.4062\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 9.4519 - categorical_accuracy: 0.3646 - val_loss: 9.6330 - val_categorical_accuracy: 0.4062\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 8.8696 - categorical_accuracy: 0.3854 - val_loss: 10.4939 - val_categorical_accuracy: 0.3125\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 7.2524 - categorical_accuracy: 0.5312 - val_loss: 11.7011 - val_categorical_accuracy: 0.3125\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 8.9408 - categorical_accuracy: 0.3958 - val_loss: 12.6346 - val_categorical_accuracy: 0.1562\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 10.1728 - categorical_accuracy: 0.2083 - val_loss: 6.4764 - val_categorical_accuracy: 0.4375\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 9.8466 - categorical_accuracy: 0.3854 - val_loss: 10.1916 - val_categorical_accuracy: 0.3125\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 14s 1s/step - loss: 9.0479 - categorical_accuracy: 0.4167 - val_loss: 11.1598 - val_categorical_accuracy: 0.3750\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 14s 1s/step - loss: 9.9484 - categorical_accuracy: 0.3333 - val_loss: 9.3115 - val_categorical_accuracy: 0.4375\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 14s 1s/step - loss: 8.4664 - categorical_accuracy: 0.5104 - val_loss: 13.0852 - val_categorical_accuracy: 0.3438\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 14s 1s/step - loss: 10.6279 - categorical_accuracy: 0.3646 - val_loss: 11.6193 - val_categorical_accuracy: 0.2812\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 16s 1s/step - loss: 11.0922 - categorical_accuracy: 0.3438 - val_loss: 11.7418 - val_categorical_accuracy: 0.3438\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 16s 1s/step - loss: 9.1067 - categorical_accuracy: 0.3646 - val_loss: 9.2419 - val_categorical_accuracy: 0.4062\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 16s 1s/step - loss: 10.4051 - categorical_accuracy: 0.3125 - val_loss: 13.8479 - val_categorical_accuracy: 0.2812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ffaf15aaf0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it, steps_per_epoch=12, validation_data=valid_it, validation_steps=4, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
